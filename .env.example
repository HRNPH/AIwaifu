HF_TOKEN=hf-<random-string>

#HF_CACHE=~/.cache/huggingface
#UID=1000
#GID=0
#TZ=UTC

PORT=7860
API_KEY=<random-string>
#SSL_KEYFILE=~/ssl/server.key
#SSL_CERTFILE=~/ssl/server.crt
#Change the following to the model you want to use
MODEL_NAME=PygmalionAI/pygmalion-2-7b
REVISION=main
DATATYPE=half # FP16. Recommended for quantization.
#KVCACHE=fp8_e5m2 # It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop.
#CONTEXT_LENGTH=2064 # If unspecified, will be automatically derived from the model.
NUM_GPUS=1
GPU_MEMORY_UTILIZATION=0.9 # If you are running out of memory, consider decreasing 'gpu_memory_utilization' or enforcing eager mode.
#QUANTIZATION=awq
#ENFORCE_EAGER=true # If you are running out of memory, consider decreasing 'gpu_memory_utilization' or enforcing eager mode.
#KOBOLD_API=true    # use this to launch a kobold compatible server in addition to the OpenAI one
CMD_ADDITIONAL_ARGUMENTS="--max-model-len 2048 --seed 0 "
HF_HUB_ENABLE_HF_TRANSFER=1 # for faster downloads
